{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahupra1357/LLM/blob/main/How_to_use_Hugging_Face_Spaces_for_deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lecture, we will learn how to use Hugging Face Spaces for deployment. Hugging Face Spaces is a platform that allows you to easily host and share your LLM models and applications with others. We will use Hugging Face Spaces to create a web app that can perform sentiment analysis on user input using our fine-tuned BERT model.\n",
        "\n",
        "Hugging Face Spaces is a feature of Hugging Face Hub, which is a repository of LLM models and datasets. You can access Hugging Face Spaces by visiting the [Hugging Face Hub website] and clicking on the Spaces tab. You will see a list of spaces created by other users, which you can explore and interact with. You can also create your own space by clicking on the New Space button.\n",
        "\n",
        "To create a space, you need to do the following steps:\n",
        "\n",
        "Choose a name and a description for your space. The name should be unique and descriptive, and the description should explain what your space does and how to use it.\n",
        "Choose a framework and a template for your space. The framework is the technology that you will use to build your space, such as Streamlit, Gradio, or Flask. The template is the code that you will use to start your space, such as a simple text classifier, an image classifier, or a custom code. You can also choose to start from scratch if you want to write your own code.\n",
        "Edit the code and the settings of your space. The code is the main logic of your space, which you can write in Python or any other supported language. The settings are the options that you can configure for your space, such as the visibility, the collaborators, the environment variables, etc.\n",
        "Deploy and share your space. Once you are happy with your code and settings, you can deploy your space by clicking on the Deploy button. This will launch your space on a URL that you can access and share with others.\n",
        "Let’s see how we can create a space that can perform sentiment analysis on user input using our fine-tuned BERT model. We will use Streamlit as our framework and a simple text classifier as our template. We will also use Transformers and Hugging Face Datasets libraries to load and use our model and dataset.\n",
        "\n",
        "First, we need to choose a name and a description for our space. We can name it sentiment-analysis-with-bert and describe it as A web app that performs sentiment analysis on user input using a fine-tuned BERT model.\n",
        "\n",
        "Next, we need to choose a framework and a template for our space. We can choose Streamlit as our framework and a simple text classifier as our template. Streamlit is a library that allows you to create interactive web apps with Python code. A simple text classifier is a template that allows you to classify text into categories using a LLM model.\n",
        "\n",
        "Then, we need to edit the code and the settings of our space. The code is already written for us by the template, but we need to make some changes to adapt it to our task and model. We can edit the code by using the online editor provided by Hugging Face Spaces. We can also use the terminal window to install any additional libraries or packages that we need.\n",
        "\n",
        "The code consists of three main parts:\n",
        "\n",
        "Importing the libraries and modules that we need for our app\n",
        "Loading the model and the tokenizer that we fine-tuned for our task\n",
        "Creating the user interface and the logic of our app\n",
        "We can edit each part as follows:\n",
        "\n",
        "Importing the libraries and modules that we need for our app\n",
        "We need to import Streamlit, Transformers, and Hugging Face Datasets libraries, as well as some other modules that we will use later. We can do this by adding the following lines at the beginning of our code:"
      ],
      "metadata": {
        "id": "wzgZNauHc_h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Streamlit library\n",
        "import streamlit as st\n",
        "\n",
        "# Import Transformers library\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Import Hugging Face Datasets library\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import numpy library\n",
        "import numpy as np\n",
        "\n",
        "# Import torch library\n",
        "import torch"
      ],
      "metadata": {
        "id": "6nDeucqwdAnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-qMHmX-dQgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model and the tokenizer that we fine-tuned for our task\n",
        "We need to load the model and the tokenizer that we fine-tuned for our task from our output directory. We can do this by using the AutoModelForSequenceClassification and AutoTokenizer classes from the Transformers library. We can also define some constants that we will use later, such as the labels, the maximum length, and the device. We can do this by adding the following lines after importing the libraries:"
      ],
      "metadata": {
        "id": "OFuM-tsPdD4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine-tuned BERT model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('output')\n",
        "\n",
        "# Load fine-tuned BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('output')\n",
        "\n",
        "# Define labels\n",
        "labels = ['Negative', 'Positive']\n",
        "\n",
        "# Define maximum length\n",
        "max_length = 128\n",
        "\n",
        "# Define device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Sd8NYrD_dHUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the user interface and the logic of our app\n",
        "We need to create the user interface and the logic of our app using Streamlit functions. We can do this by adding the following lines at the end of our code:"
      ],
      "metadata": {
        "id": "GheGLAjHdLih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a title for the app\n",
        "st.title('Sentiment Analysis with BERT')\n",
        "\n",
        "# Create a text input for the user\n",
        "text = st.text_input('Enter some text')\n",
        "\n",
        "# Create a button to perform sentiment analysis\n",
        "if st.button('Analyze'):\n",
        "  # Encode the text into tokens\n",
        "  encoded = tokenizer(text, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "  # Move the tokens to the device\n",
        "  encoded = encoded.to(device)\n",
        "  # Get the model outputs\n",
        "  outputs = model(**encoded)\n",
        "  # Get the logits\n",
        "  logits = outputs.logits\n",
        "  # Get the probabilities\n",
        "  probs = torch.softmax(logits, dim=-1)\n",
        "  # Get the predicted label\n",
        "  pred = torch.argmax(probs, dim=-1)\n",
        "  # Get the predicted label name\n",
        "  pred_name = labels[pred.item()]\n",
        "  # Get the confidence score\n",
        "  score = probs[pred].item()\n",
        "  # Display the result\n",
        "  st.write(f'The sentiment is {pred_name} with a confidence score of {score:.2f}')"
      ],
      "metadata": {
        "id": "IgnUWWL0dSn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to deploy and share our space. Once we are happy with our code and settings, we can deploy our space by clicking on the Deploy button. This will launch our space on a URL that we can access and share with others.\n",
        "\n",
        "We can test our space by entering some text and clicking on the Analyze button. We will see the sentiment and the confidence score of our text displayed on the screen. For example, if we enter “The movie was very boring and predictable.”, we will see “The sentiment is Negative with a confidence score of 0.99”.\n",
        "\n",
        "In this lecture, we learned how to use Hugging Face Spaces for deployment. We learned how to create a web app that can perform sentiment analysis on user input using our fine-tuned BERT model. We also learned how to use Hugging Face Spaces tools and resources to simplify and optimize our deployment process.\n",
        "\n",
        "In the next lecture, we will learn how to use Hugging Face Inference API for inference. Hugging Face Inference API is a service that allows you to easily access and use your LLM models and applications through an API endpoint. We will see how we can use Hugging Face Inference API to perform sentiment analysis on user input using our fine-tuned BERT model. See you in the next lecture."
      ],
      "metadata": {
        "id": "wVW87StidXam"
      }
    }
  ]
}