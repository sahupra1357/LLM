{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahupra1357/LLM/blob/main/How_to_use_Transformers_for_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lecture, we will learn how to use Transformers for fine-tuning. Transformers is a library that provides state-of-the-art LLM models and tools for natural language processing. We will use Transformers to load and fine-tune a pre-trained LLM model for our sentiment analysis task.\n",
        "\n",
        "There are many LLM models available in Transformers, such as GPT-3, BERT, and T5. Each model has its own architecture, pre-training objective, and vocabulary. For our task, we will use BERT as our LLM model. BERT stands for Bidirectional Encoder Representations from Transformers, which is a model that can encode both left and right context of a text and learn from masked words. BERT has shown impressive results in various natural language processing tasks, such as text classification, question answering, and text generation.\n",
        "\n",
        "To use BERT for our task, we need to do the following steps:\n",
        "\n",
        "Import the Transformers library and the Hugging Face Datasets library. The Hugging Face Datasets library provides easy access to various datasets for natural language processing. We will use it to load our SST-2 dataset.\n",
        "Load the pre-trained BERT model and tokenizer from the Transformers library. The tokenizer is a tool that converts text into numerical tokens that can be fed into the model. We will use the bert-base-uncased model and tokenizer, which are trained on lower-cased English text.\n",
        "Load the SST-2 dataset from the Hugging Face Datasets library. The dataset consists of train, validation, and test splits, each containing movie reviews and labels.\n",
        "Preprocess the dataset by using the tokenizer to encode the text and labels into tensors that can be fed into the model. We will also truncate or pad the sequences to a maximum length of 128 tokens.\n",
        "Define the training arguments by using the TrainingArguments class from the Transformers library. The training arguments specify various parameters and settings for training and evaluation, such as batch size, learning rate, number of epochs, logging steps, output directory, etc.\n",
        "Define the trainer by using the Trainer class from the Transformers library. The trainer is a tool that provides a simple and efficient way to train and evaluate LLM models. We will pass the model, the dataset, and the training arguments to the trainer.\n",
        "Train the model by calling the train method of the trainer. This will fine-tune the model on the train split of the dataset and save the best model checkpoint in the output directory.\n",
        "Evaluate the model by calling the evaluate method of the trainer. This will evaluate the model on the test split of the dataset and return a dictionary of metrics, such as accuracy, precision, recall, etc.\n",
        "Letâ€™s see how we can implement these steps in Python code. First, we need to import the Transformers library and the Hugging Face Datasets library. We can do this by running the following code:"
      ],
      "metadata": {
        "id": "MZTUGJRFXCkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Transformers library\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Import Hugging Face Datasets library\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "05SjHRrpXQWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to load the pre-trained BERT model and tokenizer from the Transformers library. We can do this by using the AutoModelForSequenceClassification and AutoTokenizer classes from the Transformers library. These classes can automatically load any LLM model and tokenizer from a given name or path. We will use the bert-base-uncased model and tokenizer, which are trained on lower-cased English text. We can load them by running the following code:"
      ],
      "metadata": {
        "id": "14cGQbLhXiBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "U3xyU7eOXRCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to load the pre-trained BERT model and tokenizer from the Transformers library. We can do this by using the AutoModelForSequenceClassification and AutoTokenizer classes from the Transformers library. These classes can automatically load any LLM model and tokenizer from a given name or path. We will use the bert-base-uncased model and tokenizer, which are trained on lower-cased English text. We can load them by running the following code:"
      ],
      "metadata": {
        "id": "npoS1xPTXzsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "B7F0GPoZX1ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we need to load the SST-2 dataset from the Hugging Face Datasets library. We can do this by using the load_dataset function from the Hugging Face Datasets library. This function can automatically load any dataset from a given name or path. We will use the glue name with sst2 as a configuration name to load our SST-2 dataset. The dataset consists of train, validation, and test splits, each containing movie reviews and labels. We can load them by running the following code:"
      ],
      "metadata": {
        "id": "X3hn_1IJX1HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SST-2 dataset\n",
        "dataset = load_dataset('glue', 'sst2')"
      ],
      "metadata": {
        "id": "nFxi2tE6X-19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to preprocess the dataset by using the tokenizer to encode the text and labels into tensors that can be fed into the model. We can do this by using a custom function that takes a batch of examples as input and returns a batch of encoded examples as output. We can also truncate or pad the sequences to a maximum length of 128 tokens by using the max_length and padding arguments of the tokenizer. We can define and apply this function by running the following code:"
      ],
      "metadata": {
        "id": "kX-HPAWdYGNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess the dataset\n",
        "def preprocess(batch):\n",
        "  # Encode the text and labels into tensors\n",
        "  encoded = tokenizer(batch['sentence'], max_length=128, padding='max_length', truncation=True)\n",
        "  # Add the labels to the encoded dictionary\n",
        "  encoded['labels'] = batch['label']\n",
        "  # Return the encoded dictionary\n",
        "  return encoded\n",
        "\n",
        "# Apply the function to the dataset\n",
        "dataset = dataset.map(preprocess, batched=True)"
      ],
      "metadata": {
        "id": "Av_0EG1TYNHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we need to define the training arguments by using the TrainingArguments class from the Transformers library. The training arguments specify various parameters and settings for training and evaluation, such as batch size, learning rate, number of epochs, logging steps, output directory, etc. We can define them by running the following code:"
      ],
      "metadata": {
        "id": "e8Jww8y9YQzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir='output', # Output directory\n",
        "  num_train_epochs=3, # Number of training epochs\n",
        "  per_device_train_batch_size=32, # Batch size per device during training\n",
        "  per_device_eval_batch_size=32, # Batch size for evaluation\n",
        "  warmup_steps=500, # Number of warmup steps for learning rate scheduler\n",
        "  weight_decay=0.01, # Strength of weight decay\n",
        "  logging_dir='logs', # Directory for storing logs\n",
        "  logging_steps=10, # Log every X updates steps\n",
        "  evaluation_strategy='steps', # Evaluation strategy to adopt during training\n",
        "  eval_steps=50, # Evaluation step\n",
        ")"
      ],
      "metadata": {
        "id": "ldlOlmFHYab0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to define the trainer by using the Trainer class from the Transformers library. The trainer is a tool that provides a simple and efficient way to train and evaluate LLM models. We will pass the model, the dataset, and the training arguments to the trainer. We can define it by running the following code:"
      ],
      "metadata": {
        "id": "Vc0rvbO4Yezk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "  model=model, # The model to train\n",
        "  args=training_args, # Training arguments\n",
        "  train_dataset=dataset['train'], # Training dataset\n",
        "  eval_dataset=dataset['test'], # Evaluation dataset\n",
        ")"
      ],
      "metadata": {
        "id": "VVOyHZRXYjui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to train and evaluate the model by calling the train and evaluate methods of the trainer. These methods will fine-tune and evaluate the model on the train and test splits of the dataset respectively. They will also save the best model checkpoint in the output directory and return a dictionary of metrics. We can train and evaluate the model by running the following code:"
      ],
      "metadata": {
        "id": "tjLvfOnrYqW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "pA5_07uPYwMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we run this code, we may get an output like this:"
      ],
      "metadata": {
        "id": "2YiuEMiSYzgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{'epoch': 3.0,\n",
        " 'eval_accuracy': 0.9210526315789473,\n",
        " 'eval_loss': 0.24363629519939423,\n",
        " 'eval_runtime': 1.6719,\n",
        " 'eval_samples_per_second': 358.726,\n",
        " 'eval_steps_per_second': 29.894}"
      ],
      "metadata": {
        "id": "iHA-Lv2DY3MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, our fine-tuned BERT model achieved an accuracy of about 92% on the test set of SST-2 dataset, which is quite impressive.\n",
        "\n",
        "In this lecture, we learned how to use Transformers for fine-tuning. We learned how to load and fine-tune a pre-trained BERT model for our sentiment analysis task. We also learned how to use Transformers tools and resources to simplify and optimize our fine-tuning process.\n",
        "\n",
        "In the next lecture, we will learn how to use Hugging Face Spaces for deployment. Hugging Face Spaces is a platform that allows you to easily host and share your LLM models and applications with others. We will see how we can use Hugging Face Spaces to create a web app that can perform sentiment analysis on user input using our fine-tuned BERT model. See you in the next lecture."
      ],
      "metadata": {
        "id": "zp3UnIMWY8WA"
      }
    }
  ]
}